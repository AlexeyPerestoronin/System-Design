# Hadoop
**Hadoop** — это фундаментальная платформа эры Big Data, которая решила проблему хранения и пакетной обработки огромных массивов разнородных данных на дешёвом железе. Хотя её ядро (MapReduce) уступает в скорости более новым инструментам (Spark), её экосистема (особенно HDFS, YARN, Hive, HBase) и концепции остаются критически важными для построения масштабируемых систем обработки данных. Современные Data Lake и аналитические платформы часто используют компоненты Hadoop (особенно Hive, HBase) вместе с облачными хранилищами и Spark.

## Классификация
Hadoop относится к следующим категориям:
* Платформа для Big Data:  
    Обеспечивает фундамент для хранения (HDFS) и вычислений (MapReduce, YARN).
* Распределённая файловая система:
    Основное хранилище — HDFS (Hadoop Distributed File System), которое разбивает файлы на блоки и распределяет их по узлам кластера.
* Пакетная обработка данных:
    Изначально ядро вычислений — фреймворк MapReduce, оптимизированный для обработки огромных наборов данных длительными заданиями (часы, минуты), но не для мгновенных запросов.
* Экосистема: Помимо ядра (HDFS, YARN, MapReduce), включает множество проектов:
  * HBase: Уже колоночная NoSQL БД для реального времени поверх HDFS.
  * Hive: Система для запросов на SQL-подобном языке (HiveQL), преобразующая их в задания MapReduce/Tez/Spark.
  * Spark: Быстрый движок для обработки данных (в памяти), который может работать поверх HDFS и использовать YARN.
  * Pig: Язык высокого уровня для создания потоков данных.
  * ZooKeeper: Сервис координации для распределённых систем.
  * Sqoop, Flume: Инструменты для импорта данных.

## Преимущества
* Масштабируемость (Scale-Out):  
    Легко масштабируется горизонтально путём добавления недорогих серверов в кластер. Может обрабатывать петабайты данных.
* Отказоустойчивость:  
    Данные автоматически реплицируются (по умолчанию 3 копии) на разные узлы. При отказе оборудования обработка продолжается.
* Гибкость (Schema-on-Read): 
    Можно хранить данные любой структуры (структурированные, полуструктурированные, неструктурированные — логи, JSON, видео, текст). Схема накладывается при чтении, что ускоряет загрузку.
* Экономичность:  
    Построена на использовании стандартного, а не дорогого специализированного оборудования.
* Вычислительная мощность:  
    Модель MapReduce позволяет обрабатывать данные параллельно на сотнях и тысячах узлов.
* Большая экосистема:  
    Богатый набор инструментов (Hive, Spark, HBase и др.) для разных задач.

## Недостатки
* Не является СУБД реального времени:  
    Традиционный MapReduce не подходит для интерактивных запросов или потоковой обработки с низкой задержкой.  
    Это решается Spark, HBase.
* Сложность:  
    Экосистема Hadoop требует глубоких знаний для настройки, оптимизации и управления.
* Высокие накладные расходы:  
    Для маленьких объёмов данных накладные расходы на распределённую координацию делают Hadoop неэффективным. Он раскрывается на больших данных.
* Сложность программирования:  
    Написание нативных заданий MapReduce на Java трудоёмко.
    Исправляется использованием Hive, Pig, Spark.
* Проблемы с безопасностью:  
    Изначально безопасность была слабым местом, но сейчас развивается (Kerberos, Apache Ranger).
* Не для частых изменений данных:  
    HDFS оптимизирована для однократной записи и многократного чтения.  
    Частые обновления файлов неэффективны (для этого есть HBase).

## Сфера применения:
* Хранение и анализ больших логов:  
    Анализ поведения пользователей, кликов, поисковых запросов, телеметрии.
* Анализ больших объёмов неструктурированных данных:  
    Обработка текстов, изображений (компьютерное зрение), аудио для ML-моделей.
* Data Lake (Озёр данных):  
    Хранилище "сырых" данных в первоначальном виде для последующего разведочного анализа.
* Пакетная аналитика (ETL):  
    Очистка, трансформация и агрегация данных для загрузки в традиционные хранилища данных (Data Warehouse).
* Рекомендательные системы:  
    Анализ истории покупок и поведения для построения рекомендаций (часто с использованием ML-библиотек Spark).
* Финтех и риск-аналитик:  
    Анализ транзакций, выявление мошеннических схем.

## Эволюция и современный контекст
Сегодня архитектура "чистого Hadoop" (HDFS + MapReduce) используется реже. Её вытесняют более современные подходы:
* Облачные объектные хранилища (Amazon S3, Azure Blob, Google Cloud Storage) часто заменяют HDFS как более дешёвое и управляемое хранилище для Data Lake.
* Фреймворк Apache Spark стал де-факто стандартом для обработки данных в экосистеме Hadoop благодаря скорости работы в памяти и более удобному API.
* Контейнеризация (Kubernetes): YARN как менеджер ресурсов конкурирует с Kubernetes для оркестрации рабочих нагрузок Big Data.
